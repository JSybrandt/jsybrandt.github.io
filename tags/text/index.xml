<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Text on Justin Sybrandt</title>
    <link>http://sybrandt.com/tags/text/</link>
    <description>Recent content in Text on Justin Sybrandt</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Justin Sybrandt</copyright>
    <lastBuildDate>Wed, 27 Sep 2017 14:18:52 -0400</lastBuildDate>
    <atom:link href="/tags/text/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Document Embedding</title>
      <link>http://sybrandt.com/post/document-embedding/</link>
      <pubDate>Wed, 27 Sep 2017 14:18:52 -0400</pubDate>
      
      <guid>http://sybrandt.com/post/document-embedding/</guid>
      <description>

&lt;p&gt;In a &lt;a href=&#34;http://sybrandt.com/post/word-embeddings/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; I talked about how tools like &lt;em&gt;word2vec&lt;/em&gt; are used to numerically understand the meanings behind words.
In this post, I&amp;rsquo;m going to continue that discussion by describing ways we can find numerical representations for whole documents.
So, I&amp;rsquo;ll be assuming you&amp;rsquo;re already familiar with the concept of word embeddings.&lt;/p&gt;

&lt;h2 id=&#34;why-do-we-need-document-embeddings&#34;&gt;Why do we need document embeddings?&lt;/h2&gt;

&lt;p&gt;Many real-world applications need to understand the content of text which is longer than just a single word.
For example, imagine you wanted to find all the political tweets on twitter.
Well, the first thing you might try is to make a big list of all the words you felt were &amp;ldquo;political.&amp;rdquo;
You might list &amp;ldquo;president,&amp;rdquo; &amp;ldquo;congress,&amp;rdquo; and &amp;ldquo;senate&amp;rdquo; and simply search for any tweet that contained those words.&lt;/p&gt;

&lt;p&gt;Of course, this would work for many cases, but you would miss tweets that don&amp;rsquo;t use the words on your list.
For example, image a tweet that used a congressman&amp;rsquo;s name, but didn&amp;rsquo;t actually write the word &amp;ldquo;congress.&amp;rdquo;
Also, you would get tweets that use a word on your list, but aren&amp;rsquo;t really what you&amp;rsquo;re looking for.
For example, I just learned that one should refer to a group of small colorful lizards as a &amp;ldquo;&lt;em&gt;congress&lt;/em&gt; of salamanders.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Another application that can be very important is that of detecting duplicates.
For instance, when you post on &lt;a href=&#34;https://stackoverflow.com/&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;, there is a feature which attempts to find questions similar to the one you are asking.
If we simply try to look for the number of words shared between posts then we will have very similar problems to the twitter example above.&lt;/p&gt;

&lt;p&gt;In order to solve these problems we are going to use the concepts from word embeddings to learn a vector representation of each document.
This will give us an approximation of that document&amp;rsquo;s content, and that will give us a sophisticated way to compare and classify text.&lt;/p&gt;

&lt;h2 id=&#34;how-do-we-get-a-document-s-embedding&#34;&gt;How do we get a document&amp;rsquo;s embedding?&lt;/h2&gt;

&lt;p&gt;Of course, there are many different ways to find a vector representation of a document.
Here we are going to summarize three, the BOW method, the centroid method, and the doc2vec methods.&lt;/p&gt;

&lt;h3 id=&#34;bag-of-words&#34;&gt;Bag Of Words&lt;/h3&gt;

&lt;p&gt;The simplest embedding is still the Bag-Of-Words method.
Like before, if we have $|W|$ different words in our corpus, then we will need a vector $R$ of length $|W|$ to represent our document.
We assume that each word has some index $i$, so our vector $R(d)_i = \text{# of times $w_i$ occurs in $d$}$.&lt;/p&gt;

&lt;p&gt;Comparing documents by their BOW vectors is a start, but we sill have all of the same problems we talked about above and in the previous post.&lt;/p&gt;

&lt;h3 id=&#34;centroids&#34;&gt;Centroids&lt;/h3&gt;

&lt;p&gt;If you have been following closely, you probably already thought of this.
If we have a word embedding $R(w_i)$ for each of our words $w_i \in W$, then why not just represent a document as the average of all of its contained words?&lt;/p&gt;

&lt;p&gt;$$ R(d_j) = \frac{\sum\limits_{w_i \in W_j} R(w_i)}{|W_j|}$$&lt;/p&gt;

&lt;p&gt;This method actually works pretty well, especially for smaller documents&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.
If we think about comparing tweets, like we did above, then we would expect the vectors of two political tweets to be close to each other.
It doesn&amp;rsquo;t matter if these tweets don&amp;rsquo;t use exactly the same words, one might be talking about a prominent politician and the other might be talking about a specific policy, but they both will likely use words that often occur in a &lt;em&gt;political context&lt;/em&gt;.
If one tweet were really talking about a congress of salamanders, then we would expect vector for salamander would move that tweet&amp;rsquo;s centroid further away from all the political tweets.&lt;/p&gt;

&lt;p&gt;So whats the downside?
Well, just like BOW, we lose the ordering of the words.
This means &amp;ldquo;cats eat mice&amp;rdquo; and &amp;ldquo;mice eat cats&amp;rdquo; will have the same centroid.
Additionally, we still fail to disambiguate the same word being used in different contexts.
This means that &amp;ldquo;I swung the bat&amp;rdquo; and &amp;ldquo;I swung at the bat&amp;rdquo; will only differ slightly.&lt;/p&gt;

&lt;h3 id=&#34;doc2vec&#34;&gt;Doc2Vec&lt;/h3&gt;

&lt;p&gt;Yup, the moment problems arise, we make a new &lt;em&gt;blank2vec&lt;/em&gt;.
Doc2Vec&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; learns vector representations of paragraphs along with word representations.
By combining these ideas, Doc2Vec embeddings outperformed many of the other methods that were around when it debuted&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;If you remember word2vec, we learned the representation of a word by looking at its surrounding context.
Doc2Vec adds a new vector into this context, representing the paragraph.
In the same way word2vec refers to both the CBOW and skip-gram method, doc2vec refers to two methods, each with their pros and cons.&lt;/p&gt;

&lt;h3 id=&#34;distributed-memory-model-pv-dm&#34;&gt;Distributed Memory Model (PV-DM)&lt;/h3&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://sybrandt.com/img/posts/doc2vec.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;PV-DM training diagram. Source: Le &amp;amp; Mikolov 2014&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;In PV-DM, each time we attempt to predict a word given it&amp;rsquo;s context, we introduce a vector corresponding the paragraph from which that window was taken.
Although it would require a little more engineering, this is the mechanisms we might use if we wanted to disambiguate &amp;ldquo;I swung the bat&amp;rdquo; and &amp;ldquo;I swung at the bat&amp;rdquo;.
More directly, the paragraph vector helps inform the model as to which concepts are most likely to be discussed within the current window.&lt;/p&gt;

&lt;p&gt;Le, in the original paper&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, says that this vector provides &lt;em&gt;memory&lt;/em&gt;, because the vector will change based on the words present in other parts of the same paragraph.
Conceptually, imagine a paragraph discussing a man and his dog.
Because of the paragraph vector&amp;rsquo;s influence, the model will be more likely to succeed when attempting to fill in the blank in: &amp;ldquo;the man walked his ____.&amp;rdquo;&lt;/p&gt;

&lt;h3 id=&#34;distributed-bag-of-words-pv-dbow&#34;&gt;Distributed Bag of Words (PV-DBOW)&lt;/h3&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://sybrandt.com/img/posts/doc2vec%28b%29.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;PV-DBOW training diagram. Source: Le &amp;amp; Mikolov 2014&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;Another way to find these vectors is the PV-DBOW method, where a paragraph vector is used to predict a number of words which have been randomly sampled from its text.
Although this model starts with a single element and predicts a number, it seems to resemble the skip-gram method in word2vec.
This is a bit inaccurate because the training process does not consider the ordering of these predicted words.
Additionally, in comparison to PV-DM, this method is computationally less expensive.&lt;/p&gt;

&lt;p&gt;Le&amp;rsquo;s paper states that the PV-DBOW method tends to produce poorer results than the PV-DM method, but when used in combination with PV-DM provides more consistent results.&lt;/p&gt;

&lt;h2 id=&#34;comparison-and-conclusions&#34;&gt;Comparison and Conclusions&lt;/h2&gt;

&lt;p&gt;So when doc2vec initially premiered, there was some controversy as to whether it could really outperform the centroid method.
So, Lau and Baldwin performed an extensive comparison between these methodologies across different domains&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.
The general consensus what that different methods are best suited for different tasks.
For example, centroids perform well on tweets, but are outperformed on longer documents.
I highly recommend this paper to anyone looking to include some of these techniques into their own work.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;When comparing these methodologies, I&amp;rsquo;m referring to &lt;a href=&#34;https://arxiv.org/abs/1607.05368&#34; target=&#34;_blank&#34;&gt;this great paper&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;Doc2Vec was also worked on by the original word2vec author, and it is also a &lt;a href=&#34;https://arxiv.org/abs/1405.4053&#34; target=&#34;_blank&#34;&gt;very good paper&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Word Embedding Basics</title>
      <link>http://sybrandt.com/post/word-embeddings/</link>
      <pubDate>Sun, 17 Sep 2017 14:18:26 -0400</pubDate>
      
      <guid>http://sybrandt.com/post/word-embeddings/</guid>
      <description>

&lt;p&gt;Recently, in text mining circles, a new method of representing words has taken off.
This has been due, in a large part, to recent papers from Mikolov et al. and tools like &lt;em&gt;word2vec&lt;/em&gt; &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.
Since then, many other projects have applied this concept to a wide variety of areas within data mining &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.
So what is all the hype about? What are these embeddings and why do we need them?&lt;/p&gt;

&lt;h2 id=&#34;what-is-a-word-embedding&#34;&gt;What is a word embedding?&lt;/h2&gt;

&lt;p&gt;So the word &amp;ldquo;embedding&amp;rdquo; is vague, and often times the same thing will be referred to as a &amp;ldquo;representation.&amp;rdquo;
These terms clearly aren&amp;rsquo;t clear.
When we say we want to find &amp;ldquo;word embeddings&amp;rdquo; or &amp;ldquo;word representations&amp;rdquo; we mean that we want to find numerical representations for a set of words from a set of documents.
Of course, the question is &amp;ldquo;how?&amp;rdquo;
Or maybe, the question is &amp;ldquo;why?&amp;rdquo;&lt;/p&gt;

&lt;h2 id=&#34;why&#34;&gt;&amp;ldquo;Why?&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;Well, text is very difficult for a computer to process.
People write in very expressive ways, and the meanings of words change over time.
Because of this, there are very few machine learning methods which accept &amp;ldquo;text&amp;rdquo; as input.
On the other hand, practically every method accepts a vector as input.&lt;/p&gt;

&lt;p&gt;So imagine you wanted to train a classifier to distinguish between happy and sad news stories.
Data mining techniques such as Support vector Machines (SVM) and Neural Networks (NN) are already very good at solving classification problems.
So in order to use them, we need to convert our plain text into input that these methods can accept.&lt;/p&gt;

&lt;h2 id=&#34;okay-so-how&#34;&gt;&amp;ldquo;Okay, so how?&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;Before I get into the methods, I want to define some notation to make the explanations a bit more clear:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$W$ is our set of words.

&lt;ul&gt;
&lt;li&gt;A single word is $w_i$.&lt;/li&gt;
&lt;li&gt;$|W|$ is the number of different words we have.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;$D$ is our set of documents.

&lt;ul&gt;
&lt;li&gt;A single document is $d_j$.&lt;/li&gt;
&lt;li&gt;$|D|$ is the number of different words we have.&lt;/li&gt;
&lt;li&gt;Each document $d_j$ is comprised of its own sequence of words $W_j$.&lt;/li&gt;
&lt;li&gt;$W_j[k]$ is the $k^{\text{th}}$ word in document $d_j$&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;$R(x)$ is a function that takes a word or document and produces a vector.

&lt;ul&gt;
&lt;li&gt;A vector of length $l$ is written as $\Re^l$.&lt;/li&gt;
&lt;li&gt;$R(x)_k$ is the $k^{\text{th}}$ number in the vector $R(x)$.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Okay, with that out of the way:&lt;/p&gt;

&lt;h3 id=&#34;the-old-bag-of-words&#34;&gt;The Old: Bag of Words&lt;/h3&gt;

&lt;p&gt;The easiest representation is the &lt;em&gt;bag of words&lt;/em&gt; (BOW) method, but it&amp;rsquo;s usefulness is limited.
The BOW method represents each word $w_i$ as a vector in $\Re^{|W|}$ where $R(w_i)_i = 1$ and all other values are 0.
This method represents a document by simply summing together all of its word&amp;rsquo;s vectors.&lt;/p&gt;

&lt;p&gt;$$ R(d_j) = \sum_{w_i \in W_j} R(w_i) $$&lt;/p&gt;

&lt;p&gt;This method has some pros and some critical cons.
On the pro side, if you want to know how often $w_i$ occurs in $d_j$ all you have to do is look at $R(d_j)_i$.
Also, documents that use the same words will have vectors with a higher &lt;a href=&#34;https://en.wikipedia.org/wiki/Cosine_similarity&#34; target=&#34;_blank&#34;&gt;cosine similarity&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;On the con side, this method throws away the ordering of words within a document.
For example, the sentence &lt;em&gt;&amp;ldquo;the man walks the dog&amp;rdquo;&lt;/em&gt; and &lt;em&gt;&amp;ldquo;the dog walks the man&amp;rdquo;&lt;/em&gt; have the same BOW representation.
This additionally means that if two documents use &lt;em&gt;similar&lt;/em&gt; but not the same words, they will be seen as different as two documents using completely different words.
Also, the vectors are inefficiently long, each as long as the entire vocabulary (typically tens of millions of words).
This means that one of those classifiers, such as a neural net, will have to accept really really long input vectors.
Long story short, this doesn&amp;rsquo;t scale well &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&#34;the-new-word2vec&#34;&gt;The New: Word2Vec&lt;/h3&gt;

&lt;p&gt;The new method which solves the problems with BOW is referred to as &lt;em&gt;&amp;ldquo;word2vec&amp;rdquo;&lt;/em&gt;, which is the name of the first tool which implements Mikolov&amp;rsquo;s ideas &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.
This method creates vectors in a much lower dimensionality (typically, $100 \leq l \leq 600$).
Additionally, words which are similar will tend to have similar vectors.
As a side effect, the distance between words which have the same relationship (i.e. the distance between a US state and it&amp;rsquo;s capitol) will all be about the same.
These improvements have directly led to a huge improvement in text mining over the last five years.&lt;/p&gt;

&lt;p&gt;The guiding assumption in the word2vec method is that &lt;em&gt;similar words share similar company&lt;/em&gt;.
For example, think of all the words that can fill in the blank in this sentence:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The cat __ on the sofa.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You probably thought of words like &amp;ldquo;sat,&amp;rdquo; &amp;ldquo;slept,&amp;rdquo; &amp;ldquo;jumped,&amp;rdquo; or &amp;ldquo;threw-up.&amp;rdquo;
We see that all of these words are verbs, in the past tense, and actions a cat could take.
Note that the context of the blank has a huge, albeit inexact, bearing on what word the blank could take.
By learning these sorts of context-sensitive relationships, word2vec is able to train its embeddings.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://sybrandt.com/img/posts/word2vec.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Typical word2vec Training Diagram. Source: deeplearning4j.org&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;The above figure depicts the two training methods used in word2vec to learn word embeddings.
On the left is the Continuous Bag of Words (CBOW) method, and on the right is Skip-Gram.
At a high level, the CBOW  and skip-gram model are opposites.
The former learns to predict a word given that words context, the same way we did the example about the cat above.
The latter learns to predict context given a word.&lt;/p&gt;

&lt;p&gt;Both of these use some principles from convolutional neural nets, which are outside the scope of this article&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.
All  you need is to think of these processes as black boxes that take vectors as input, and refine their result quality by comparing their output with our existing vectors.&lt;/p&gt;

&lt;p&gt;Initially, all words are initialized as a random vector.
Then we start scanning through all of the sentences in our training corpus.
For each word, we pull out a number of that word&amp;rsquo;s neighbors.
Then, using either method, we attempt to find word representations.
We use the error between our predicted vectors and the randomly initialized ones in order to improve our model.
We also occasionally swap a word&amp;rsquo;s representation with the output of our model.&lt;/p&gt;

&lt;p&gt;By this process, word2vec eventually both finds a model which can predict word vectors given context (or vice-versa) as well as a set of word vectors that maximize the quality of that predictive model.
This gives us $R(x)$, for all our words, which we can use in a number of ways to find embeddings for our documents.
One of the simplest ways to do this is by simply averaging all of a document&amp;rsquo;s word vectors together.&lt;/p&gt;

&lt;p&gt;$$ R(d_j) = \frac{\sum_{w_i \in W_j} R(w_i) )}{ |W_j| } $$&lt;/p&gt;

&lt;h2 id=&#34;so-now-what&#34;&gt;So now what?&lt;/h2&gt;

&lt;p&gt;Okay, so now that we have the fancy word vectors, we can do a lot of things with them.
For example, imagine we wanted to find news articles related to Google.
Well, we could find a large number of them by selecting documents whose vector representation is close to $R(\text{Google})$.&lt;/p&gt;

&lt;p&gt;By using these representations, we can so improve the quality of our classifiers.
Anywhere we used to use a BOW representation, we can replace that with a more efficient and more descriptive vector.
Adding to the benefits, the length of the word2vec embeddings does not change based on the size of our vocabulary.&lt;/p&gt;

&lt;h2 id=&#34;in-conclusion&#34;&gt;In Conclusion&lt;/h2&gt;

&lt;p&gt;I hope this helped give a background on word vectors, and at least brought you up to speed with this new method.
Certainly, there is a lot more I could go into, and a lot of work is being done to apply this principle to solve a wide range of problems.
But that will have to be the focus of a different post.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;Mikolov et al. (2013) &lt;em&gt;&amp;ldquo;Distributed representations of words and phrases and their compositionality&amp;rdquo;&lt;/em&gt; and &lt;em&gt;&amp;ldquo;Efficient estimation of word representations in vector space&amp;rdquo;&lt;/em&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;KDD&amp;rsquo;17 had a whole session devoted just to embeddings. &lt;a href=&#34;http://www.kdd.org/kdd2016/files/jm/KDD2017BookletV2.2.pdf&#34; target=&#34;_blank&#34;&gt;Search the schedule for RT8 &lt;em&gt;&amp;ldquo;Representations&amp;rdquo;&lt;/em&gt;.&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;Although, for the sake of completion, I want to note that BOW is used a lot, and there are techniques to limit the length of these vectors. The other problems are still valid though.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;There are a number of &lt;a href=&#34;https://arxiv.org/abs/1411.2738&#34; target=&#34;_blank&#34;&gt;decent papers&lt;/a&gt; which explain the math behind these methods, if you are interested.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
