<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.32.1" />
  <meta name="author" content="Justin Sybrandt">
  <meta name="description" content="Ph. D. Student : Machine Learning">

  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="/css/highlight.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
  
  
  


  

  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700%7cMerriweather%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-76162256-1', 'auto');
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  

  
  <link rel="alternate" href="http://sybrandt.com/index.xml" type="application/rss+xml" title="Justin Sybrandt">
  <link rel="feed" href="http://sybrandt.com/index.xml" type="application/rss+xml" title="Justin Sybrandt">
  

  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/apple-touch-icon.png">

  <link rel="canonical" href="http://sybrandt.com/post/document-embedding/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Justin Sybrandt">
  <meta property="og:url" content="http://sybrandt.com/post/document-embedding/">
  <meta property="og:title" content="Document Embedding | Justin Sybrandt">
  <meta property="og:description" content=""><meta property="og:image" content="http://sybrandt.com/img/headers/document.jpg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2017-09-27T14:18:52-04:00">
  
  <meta property="article:modified_time" content="2017-09-27T14:18:52-04:00">
  

  

  <title>Document Embedding | Justin Sybrandt</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Justin Sybrandt</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/resume/resume.pdf">
            
            <span>Resume</span>
          </a>
        </li>

        
        

        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  
<div class="article-header">
  <img src="/img/headers/document.jpg" class="article-banner" itemprop="image">
  
</div>



  <div class="article-container">
    <div class="article-inner">
      <h1 itemprop="name">Document Embedding</h1>

      

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2017-09-27 14:18:52 -0400 EDT" itemprop="datePublished">
      Sep 27, 2017
    </time>
  </span>

  

  
  
  <span class="middot-divider"></span>
  <a href="http://sybrandt.com/post/document-embedding/#disqus_thread"></a>
  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Document%20Embedding&amp;url=http%3a%2f%2fsybrandt.com%2fpost%2fdocument-embedding%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=http%3a%2f%2fsybrandt.com%2fpost%2fdocument-embedding%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2fsybrandt.com%2fpost%2fdocument-embedding%2f&amp;title=Document%20Embedding"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=http%3a%2f%2fsybrandt.com%2fpost%2fdocument-embedding%2f&amp;title=Document%20Embedding"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Document%20Embedding&amp;body=http%3a%2f%2fsybrandt.com%2fpost%2fdocument-embedding%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


      <div class="article-style" itemprop="articleBody">
        

<p>In a <a href="/post/word-embeddings/" target="_blank">previous post</a> I talked about how tools like <em>word2vec</em> are used to numerically understand the meanings behind words.
In this post, I&rsquo;m going to continue that discussion by describing ways we can find numerical representations for whole documents.
So, I&rsquo;ll be assuming you&rsquo;re already familiar with the concept of word embeddings.</p>

<h2 id="why-do-we-need-document-embeddings">Why do we need document embeddings?</h2>

<p>Many real-world applications need to understand the content of text which is longer than just a single word.
For example, imagine you wanted to find all the political tweets on twitter.
Well, the first thing you might try is to make a big list of all the words you felt were &ldquo;political.&rdquo;
You might list &ldquo;president,&rdquo; &ldquo;congress,&rdquo; and &ldquo;senate&rdquo; and simply search for any tweet that contained those words.</p>

<p>Of course, this would work for many cases, but you would miss tweets that don&rsquo;t use the words on your list.
For example, image a tweet that used a congressman&rsquo;s name, but didn&rsquo;t actually write the word &ldquo;congress.&rdquo;
Also, you would get tweets that use a word on your list, but aren&rsquo;t really what you&rsquo;re looking for.
For example, I just learned that one should refer to a group of small colorful lizards as a &ldquo;<em>congress</em> of salamanders.&rdquo;</p>

<p>Another application that can be very important is that of detecting duplicates.
For instance, when you post on <a href="https://stackoverflow.com/" target="_blank">Stack Overflow</a>, there is a feature which attempts to find questions similar to the one you are asking.
If we simply try to look for the number of words shared between posts then we will have very similar problems to the twitter example above.</p>

<p>In order to solve these problems we are going to use the concepts from word embeddings to learn a vector representation of each document.
This will give us an approximation of that document&rsquo;s content, and that will give us a sophisticated way to compare and classify text.</p>

<h2 id="how-do-we-get-a-document-s-embedding">How do we get a document&rsquo;s embedding?</h2>

<p>Of course, there are many different ways to find a vector representation of a document.
Here we are going to summarize three, the BOW method, the centroid method, and the doc2vec methods.</p>

<h3 id="bag-of-words">Bag Of Words</h3>

<p>The simplest embedding is still the Bag-Of-Words method.
Like before, if we have $|W|$ different words in our corpus, then we will need a vector $R$ of length $|W|$ to represent our document.
We assume that each word has some index $i$, so our vector $R(d)_i = \text{# of times $w_i$ occurs in $d$}$.</p>

<p>Comparing documents by their BOW vectors is a start, but we sill have all of the same problems we talked about above and in the previous post.</p>

<h3 id="centroids">Centroids</h3>

<p>If you have been following closely, you probably already thought of this.
If we have a word embedding $R(w_i)$ for each of our words $w_i \in W$, then why not just represent a document as the average of all of its contained words?</p>

<p>$$ R(d_j) = \frac{\sum\limits_{w_i \in W_j} R(w_i)}{|W_j|}$$</p>

<p>This method actually works pretty well, especially for smaller documents<sup class="footnote-ref" id="fnref:1"><a rel="footnote" href="#fn:1">1</a></sup>.
If we think about comparing tweets, like we did above, then we would expect the vectors of two political tweets to be close to each other.
It doesn&rsquo;t matter if these tweets don&rsquo;t use exactly the same words, one might be talking about a prominent politician and the other might be talking about a specific policy, but they both will likely use words that often occur in a <em>political context</em>.
If one tweet were really talking about a congress of salamanders, then we would expect vector for salamander would move that tweet&rsquo;s centroid further away from all the political tweets.</p>

<p>So whats the downside?
Well, just like BOW, we lose the ordering of the words.
This means &ldquo;cats eat mice&rdquo; and &ldquo;mice eat cats&rdquo; will have the same centroid.
Additionally, we still fail to disambiguate the same word being used in different contexts.
This means that &ldquo;I swung the bat&rdquo; and &ldquo;I swung at the bat&rdquo; will only differ slightly.</p>

<h3 id="doc2vec">Doc2Vec</h3>

<p>Yup, the moment problems arise, we make a new <em>blank2vec</em>.
Doc2Vec<sup class="footnote-ref" id="fnref:2"><a rel="footnote" href="#fn:2">2</a></sup> learns vector representations of paragraphs along with word representations.
By combining these ideas, Doc2Vec embeddings outperformed many of the other methods that were around when it debuted<sup class="footnote-ref" id="fnref:1"><a rel="footnote" href="#fn:1">1</a></sup>.</p>

<p>If you remember word2vec, we learned the representation of a word by looking at its surrounding context.
Doc2Vec adds a new vector into this context, representing the paragraph.
In the same way word2vec refers to both the CBOW and skip-gram method, doc2vec refers to two methods, each with their pros and cons.</p>

<h3 id="distributed-memory-model-pv-dm">Distributed Memory Model (PV-DM)</h3>


<figure >
    
        <img src="/img/posts/doc2vec.png" />
    
    
    <figcaption>
        <h4>PV-DM training diagram. Source: Le &amp; Mikolov 2014</h4>
        
    </figcaption>
    
</figure>


<p>In PV-DM, each time we attempt to predict a word given it&rsquo;s context, we introduce a vector corresponding the paragraph from which that window was taken.
Although it would require a little more engineering, this is the mechanisms we might use if we wanted to disambiguate &ldquo;I swung the bat&rdquo; and &ldquo;I swung at the bat&rdquo;.
More directly, the paragraph vector helps inform the model as to which concepts are most likely to be discussed within the current window.</p>

<p>Le, in the original paper<sup class="footnote-ref" id="fnref:2"><a rel="footnote" href="#fn:2">2</a></sup>, says that this vector provides <em>memory</em>, because the vector will change based on the words present in other parts of the same paragraph.
Conceptually, imagine a paragraph discussing a man and his dog.
Because of the paragraph vector&rsquo;s influence, the model will be more likely to succeed when attempting to fill in the blank in: &ldquo;the man walked his ____.&rdquo;</p>

<h3 id="distributed-bag-of-words-pv-dbow">Distributed Bag of Words (PV-DBOW)</h3>


<figure >
    
        <img src="/img/posts/doc2vec%28b%29.png" />
    
    
    <figcaption>
        <h4>PV-DBOW training diagram. Source: Le &amp; Mikolov 2014</h4>
        
    </figcaption>
    
</figure>


<p>Another way to find these vectors is the PV-DBOW method, where a paragraph vector is used to predict a number of words which have been randomly sampled from its text.
Although this model starts with a single element and predicts a number, it seems to resemble the skip-gram method in word2vec.
This is a bit inaccurate because the training process does not consider the ordering of these predicted words.
Additionally, in comparison to PV-DM, this method is computationally less expensive.</p>

<p>Le&rsquo;s paper states that the PV-DBOW method tends to produce poorer results than the PV-DM method, but when used in combination with PV-DM provides more consistent results.</p>

<h2 id="comparison-and-conclusions">Comparison and Conclusions</h2>

<p>So when doc2vec initially premiered, there was some controversy as to whether it could really outperform the centroid method.
So, Lau and Baldwin performed an extensive comparison between these methodologies across different domains<sup class="footnote-ref" id="fnref:1"><a rel="footnote" href="#fn:1">1</a></sup>.
The general consensus what that different methods are best suited for different tasks.
For example, centroids perform well on tweets, but are outperformed on longer documents.
I highly recommend this paper to anyone looking to include some of these techniques into their own work.</p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:1">When comparing these methodologies, I&rsquo;m referring to <a href="https://arxiv.org/abs/1607.05368" target="_blank">this great paper</a>.
 <a class="footnote-return" href="#fnref:1"><sup>^</sup></a></li>
<li id="fn:2">Doc2Vec was also worked on by the original word2vec author, and it is also a <a href="https://arxiv.org/abs/1405.4053" target="_blank">very good paper</a>
 <a class="footnote-return" href="#fnref:2"><sup>^</sup></a></li>
</ol>
</div>

      </div>

      


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/text">text</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/tutorial">tutorial</a>
  
</div>



    </div>
  </div>

</article>



<div class="article-container article-widget">
  <div class="hr-light"></div>
  <h3>Related</h3>
  <ul>
    
    <li><a href="/post/word-embeddings/">Word Embedding Basics</a></li>
    
    <li><a href="/post/google-slides-agile/">Agile Project Management in Google Sheets</a></li>
    
    <li><a href="/post/producer-consumer-openmp-cpp/">Producer and Consumer Model in C&#43;&#43;</a></li>
    
  </ul>
</div>




<div class="article-container">
  
<section id="comments">
  <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "sybrandt-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017 Justin Sybrandt &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script id="dsq-count-scr" src="//sybrandt-com.disqus.com/count.js" async></script>
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

