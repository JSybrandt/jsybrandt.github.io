<!DOCTYPE html>
<html lang="">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Justin Sybrandt, PhD.">
    <meta name="description" content="/">
    <meta name="keywords" content="developer,research,personal,resume,machine,learning">

    <meta property="og:site_name" content="Justin Sybrandt">
    <meta property="og:title" content="
  Word Embedding Basics - Justin Sybrandt
">
    <meta property="og:description" content="  Describes that mathematical basis of word2vec. A method of learning
  latent representations of natural language words.
">
    <meta property="og:type" content="website">
    <meta property="og:url" content="/posts/word_embedding/">
    <meta property="og:image" content="/">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="/posts/word_embedding/">
    <meta name="twitter:image" content="/">

    <base href="/posts/word_embedding/">
    <title>
  Word Embedding Basics - Justin Sybrandt
</title>

    <link rel="canonical" href="/posts/word_embedding/">
    
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
    
    <link rel="stylesheet" href="/css/normalize.min.css">
    <link rel="stylesheet" href="/css/style.min.css">

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    
      <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Justin Sybrandt">
      <link href="/index.xml" rel="feed" type="application/rss+xml" title="Justin Sybrandt" />
    



    <meta name="generator" content="Hugo 0.85.0" />
  </head>

  <body class="">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">Justin Sybrandt</a>
    <input type="checkbox" id="menu-control"/>
    <label class="menu-mobile  float-right " for="menu-control">
      <span class="btn-mobile  float-right ">&#9776;</span>
      <ul class="navigation-list">
        
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="/publications">Publications</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="/posts">Posts</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="/updates">Updates</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="/documents/cv.pdf">CV</a>
            </li>
          
        
        
      </ul>
    </label>
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
  <article>
    <header>
      <h1 class="title">Word Embedding Basics</h1>
      <h2 class="date">September 17, 2017</h2>
    </header>

    <p>Recently, in text mining circles, a new method of representing words has taken off.
This has been due, in a large part, to recent papers from Mikolov et al. and tools like <em>word2vec</em> <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.
Since then, many other projects have applied this concept to a wide variety of areas within data mining <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.
So what is all the hype about? What are these embeddings and why do we need them?</p>
<h2 id="what-is-a-word-embedding">What is a word embedding?</h2>
<p>So the word &ldquo;embedding&rdquo; is vague, and often times the same thing will be referred to as a &ldquo;representation.&rdquo;
These terms clearly aren&rsquo;t clear.
When we say we want to find &ldquo;word embeddings&rdquo; or &ldquo;word representations&rdquo; we mean that we want to find numerical representations for a set of words from a set of documents.
Of course, the question is &ldquo;how?&rdquo;
Or maybe, the question is &ldquo;why?&rdquo;</p>
<h2 id="why">&ldquo;Why?&rdquo;</h2>
<p>Well, text is very difficult for a computer to process.
People write in very expressive ways, and the meanings of words change over time.
Because of this, there are very few machine learning methods which accept &ldquo;text&rdquo; as input.
On the other hand, practically every method accepts a vector as input.</p>
<p>So imagine you wanted to train a classifier to distinguish between happy and sad news stories.
Data mining techniques such as Support vector Machines (SVM) and Neural Networks (NN) are already very good at solving classification problems.
So in order to use them, we need to convert our plain text into input that these methods can accept.</p>
<h2 id="okay-so-how">&ldquo;Okay, so how?&rdquo;</h2>
<p>Before I get into the methods, I want to define some notation to make the explanations a bit more clear:</p>
<ul>
<li>$W$ is our set of words.
<ul>
<li>A single word is $w_i$.</li>
<li>$|W|$ is the number of different words we have.</li>
</ul>
</li>
<li>$D$ is our set of documents.
<ul>
<li>A single document is $d_j$.</li>
<li>$|D|$ is the number of different words we have.</li>
<li>Each document $d_j$ is comprised of its own sequence of words $W_j$.</li>
<li>$W_j[k]$ is the $k^{\text{th}}$ word in document $d_j$</li>
</ul>
</li>
<li>$R(x)$ is a function that takes a word or document and produces a vector.
<ul>
<li>A vector of length $l$ is written as $\Re^l$.</li>
<li>$R(x)_k$ is the $k^{\text{th}}$ number in the vector $R(x)$.</li>
</ul>
</li>
</ul>
<p>Okay, with that out of the way:</p>
<h3 id="the-old-bag-of-words">The Old: Bag of Words</h3>
<p>The easiest representation is the <em>bag of words</em> (BOW) method, but it&rsquo;s usefulness is limited.
The BOW method represents each word $w_i$ as a vector in $\Re^{|W|}$ where $R(w_i)_i = 1$ and all other values are 0.
This method represents a document by simply summing together all of its word&rsquo;s vectors.</p>
<p>$$ R(d_j) = \sum_{w_i \in W_j} R(w_i) $$</p>
<p>This method has some pros and some critical cons.
On the pro side, if you want to know how often $w_i$ occurs in $d_j$ all you have to do is look at $R(d_j)_i$.
Also, documents that use the same words will have vectors with a higher <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a>.</p>
<p>On the con side, this method throws away the ordering of words within a document.
For example, the sentence <em>&ldquo;the man walks the dog&rdquo;</em> and <em>&ldquo;the dog walks the man&rdquo;</em> have the same BOW representation.
This additionally means that if two documents use <em>similar</em> but not the same words, they will be seen as different as two documents using completely different words.
Also, the vectors are inefficiently long, each as long as the entire vocabulary (typically tens of millions of words).
This means that one of those classifiers, such as a neural net, will have to accept really really long input vectors.
Long story short, this doesn&rsquo;t scale well <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<h3 id="the-new-word2vec">The New: Word2Vec</h3>
<p>The new method which solves the problems with BOW is referred to as <em>&ldquo;word2vec&rdquo;</em>, which is the name of the first tool which implements Mikolov&rsquo;s ideas <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.
This method creates vectors in a much lower dimensionality (typically, $100 \leq l \leq 600$).
Additionally, words which are similar will tend to have similar vectors.
As a side effect, the distance between words which have the same relationship (i.e. the distance between a US state and it&rsquo;s capitol) will all be about the same.
These improvements have directly led to a huge improvement in text mining over the last five years.</p>
<p>The guiding assumption in the word2vec method is that <em>similar words share similar company</em>.
For example, think of all the words that can fill in the blank in this sentence:</p>
<blockquote>
<p>The cat __ on the sofa.</p>
</blockquote>
<p>You probably thought of words like &ldquo;sat,&rdquo; &ldquo;slept,&rdquo; &ldquo;jumped,&rdquo; or &ldquo;threw-up.&rdquo;
We see that all of these words are verbs, in the past tense, and actions a cat could take.
Note that the context of the blank has a huge, albeit inexact, bearing on what word the blank could take.
By learning these sorts of context-sensitive relationships, word2vec is able to train its embeddings.</p>
<p><img src="/img/posts/word_embedding/word2vec.png" alt="word-embedding"></p>
<p>The above figure depicts the two training methods used in word2vec to learn word embeddings.
On the left is the Continuous Bag of Words (CBOW) method, and on the right is Skip-Gram.
At a high level, the CBOW  and skip-gram model are opposites.
The former learns to predict a word given that words context, the same way we did the example about the cat above.
The latter learns to predict context given a word.</p>
<p>Both of these use some principles from convolutional neural nets, which are outside the scope of this article<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.
All  you need is to think of these processes as black boxes that take vectors as input, and refine their result quality by comparing their output with our existing vectors.</p>
<p>Initially, all words are initialized as a random vector.
Then we start scanning through all of the sentences in our training corpus.
For each word, we pull out a number of that word&rsquo;s neighbors.
Then, using either method, we attempt to find word representations.
We use the error between our predicted vectors and the randomly initialized ones in order to improve our model.
We also occasionally swap a word&rsquo;s representation with the output of our model.</p>
<p>By this process, word2vec eventually both finds a model which can predict word vectors given context (or vice-versa) as well as a set of word vectors that maximize the quality of that predictive model.
This gives us $R(x)$, for all our words, which we can use in a number of ways to find embeddings for our documents.
One of the simplest ways to do this is by simply averaging all of a document&rsquo;s word vectors together.</p>
<p>$$ R(d_j) = \frac{\sum_{w_i \in W_j} R(w_i) )}{ |W_j| } $$</p>
<h2 id="so-now-what">So now what?</h2>
<p>Okay, so now that we have the fancy word vectors, we can do a lot of things with them.
For example, imagine we wanted to find news articles related to Google.
Well, we could find a large number of them by selecting documents whose vector representation is close to $R(\text{Google})$.</p>
<p>By using these representations, we can so improve the quality of our classifiers.
Anywhere we used to use a BOW representation, we can replace that with a more efficient and more descriptive vector.
Adding to the benefits, the length of the word2vec embeddings does not change based on the size of our vocabulary.</p>
<h2 id="in-conclusion">In Conclusion</h2>
<p>I hope this helped give a background on word vectors, and at least brought you up to speed with this new method.
Certainly, there is a lot more I could go into, and a lot of work is being done to apply this principle to solve a wide range of problems.
But that will have to be the focus of a different post.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Mikolov et al. (2013) <em>&ldquo;Distributed representations of words and phrases and their compositionality&rdquo;</em> and <em>&ldquo;Efficient estimation of word representations in vector space&rdquo;</em>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>KDD'17 had a whole session devoted just to embeddings. <a href="http://www.kdd.org/kdd2016/files/jm/KDD2017BookletV2.2.pdf">Search the schedule for RT8 <em>&ldquo;Representations&rdquo;</em>.</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Although, for the sake of completion, I want to note that BOW is used a lot, and there are techniques to limit the length of these vectors. The other problems are still valid though.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>There are a number of <a href="https://arxiv.org/abs/1411.2738">decent papers</a> which explain the math behind these methods, if you are interested.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

  </article>

  <br/>

  
      <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "sybrandt-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  
  
</section>


      </div>
      
        

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>


      
    </main>

    

  <script src="/js/app.js"></script>
  
  </body>
</html>
